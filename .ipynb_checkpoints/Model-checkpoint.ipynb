{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data_cleaned.csv\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59759 entries, 0 to 59758\n",
      "Data columns (total 15 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   BOROUGH                         59759 non-null  object \n",
      " 1   NEIGHBORHOOD                    59759 non-null  object \n",
      " 2   LOT                             59759 non-null  int64  \n",
      " 3   RESIDENTIAL UNITS               59759 non-null  int64  \n",
      " 4   COMMERCIAL UNITS                59759 non-null  int64  \n",
      " 5   TOTAL UNITS                     59759 non-null  int64  \n",
      " 6   LAND SQUARE FEET                38571 non-null  float64\n",
      " 7   GROSS SQUARE FEET               38020 non-null  float64\n",
      " 8   TAX CLASS AT TIME OF SALE       59759 non-null  int64  \n",
      " 9   SALE PRICE                      59759 non-null  float64\n",
      " 10  SALE YEAR                       59759 non-null  int64  \n",
      " 11  SALE YEAR_MONTH                 59759 non-null  object \n",
      " 12  BUILDING CLASS AT PRESENT       59759 non-null  float64\n",
      " 13  BUILDING CLASS AT TIME OF SALE  59759 non-null  int64  \n",
      " 14  AGE OF PROPERTY                 59759 non-null  float64\n",
      "dtypes: float64(5), int64(7), object(3)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 590113.9601698\ttotal: 229ms\tremaining: 22.6s\n",
      "99:\tlearn: 392854.2250291\ttotal: 1.25s\tremaining: 0us\n",
      "RMSE: 395888.4976105999\n",
      "R^2: 0.5845559811319112\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Initialize the CatBoostRegressor with categorical features\n",
    "model = CatBoostRegressor(iterations=100,\n",
    "                         learning_rate=0.1,\n",
    "                         depth=6,\n",
    "                         loss_function='RMSE',\n",
    "                         eval_metric='RMSE',\n",
    "                         random_seed=42,\n",
    "                         cat_features=categorical_cols)  # Specify categorical columns\n",
    "\n",
    "X = data.drop('SALE PRICE', axis=1)  \n",
    "y = data['SALE PRICE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Pool for training and testing data\n",
    "train_pool = Pool(X_train, y_train, cat_features=categorical_cols)\n",
    "test_pool = Pool(X_test, cat_features=categorical_cols)\n",
    "\n",
    "# Train the CatBoost model\n",
    "model.fit(train_pool, verbose=100)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(test_pool)\n",
    "\n",
    "# Calculate RMSE and R^2 scores for evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliyagizdizdaroglu/miniconda3/envs/ordinary/lib/python3.8/site-packages/lightgbm/basic.py:2034: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n",
      "/Users/aliyagizdizdaroglu/miniconda3/envs/ordinary/lib/python3.8/site-packages/lightgbm/basic.py:2054: UserWarning: categorical_feature in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n",
      "/Users/aliyagizdizdaroglu/miniconda3/envs/ordinary/lib/python3.8/site-packages/lightgbm/basic.py:2034: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n",
      "/Users/aliyagizdizdaroglu/miniconda3/envs/ordinary/lib/python3.8/site-packages/lightgbm/basic.py:2054: UserWarning: categorical_feature in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: YEAR_MONTH\n",
      "[LightGBM] [Warning] categorical_feature is set=BOROUGH,NEIGHBORHOOD,SALE, categorical_column=0,1,10 will be ignored. Current value: categorical_feature=BOROUGH,NEIGHBORHOOD,SALE\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1422\n",
      "[LightGBM] [Info] Number of data points in the train set: 47807, number of used features: 14\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 811724.945552\n",
      "[LightGBM] [Warning] Unknown parameter: YEAR_MONTH\n",
      "RMSE: 365038.0085748024\n",
      "R^2: 0.6467819272906778\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming `data` is your DataFrame and 'SALE PRICE' is the target variable\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "\n",
    "X = data.drop('SALE PRICE', axis=1)\n",
    "y = data['SALE PRICE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the LGBMRegressor, specifying categorical_feature parameter is not needed as LightGBM will auto-detect them based on dtype\n",
    "model = LGBMRegressor(num_leaves=31,\n",
    "                      learning_rate=0.1,\n",
    "                      n_estimators=100,\n",
    "                      categorical_feature=categorical_cols)  # This line is optional, LightGBM uses dtype to detect\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='rmse')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE and R^2 scores for evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Could not do the rest because of the hardware constraints, fyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate the median values for 'LAND SQUARE FEET' and 'GROSS SQUARE FEET'\n",
    "land_square_feet_median = data['LAND SQUARE FEET'].median()\n",
    "gross_square_feet_median = data['GROSS SQUARE FEET'].median()\n",
    "\n",
    "# Impute missing values using the calculated medians\n",
    "data['LAND SQUARE FEET'].fillna(land_square_feet_median, inplace=True)\n",
    "data['GROSS SQUARE FEET'].fillna(gross_square_feet_median, inplace=True)\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'target_column' is the name of your target variable\n",
    "target_column = 'SALE PRICE'  # Replace with your actual target column name\n",
    "\n",
    "# Separate features and target. The target column is not included in the transformations.\n",
    "X = data.drop(target_column, axis=1)\n",
    "y = data[target_column]\n",
    "\n",
    "# Identify categorical and numerical columns from the features only\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Define the ColumnTransformer to one-hot encode categorical variables\n",
    "# and scale numerical variables. The target variable 'y' is not transformed.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply the transformations to the features only\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Retrieve the feature names after transformations\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Proceed with the train/test split or any other operations\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train and X_test are now preprocessed and ready for model training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE and R^2 scores for evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and train the XGBoost Regression model\n",
    "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE and R^2 scores for evaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2: {r2}\")\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Check the lengths of feature_names and feature_importances\n",
    "if len(feature_names) != len(feature_importances):\n",
    "    print(f\"Length of feature_names: {len(feature_names)}\")\n",
    "    print(f\"Length of feature_importances: {len(feature_importances)}\")\n",
    "    # Raise an error or handle the mismatch accordingly\n",
    "\n",
    "# Create a DataFrame to show feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Separate features and target\n",
    "X = data_filtered[numerical_cols]\n",
    "y = data_filtered['SALE PRICE']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model (e.g., XGBoost) on the training data\n",
    "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize the SHAP explainer with your trained model and the background dataset (X_train)\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "# Compute SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize the SHAP summary plot to understand the impact of features on predictions\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After all these, my plan was to optimize the selected model using RFE method for feature selection and then perform hyperparameter tuning which I could not manage due to hardware constraints; reason being simply taking too much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
